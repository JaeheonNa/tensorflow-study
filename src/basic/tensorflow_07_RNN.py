# 순환 신경망(RNN)
#   시계열 데이터(현재의 데이터가 앞, 뒤 데이터와 연관 관계를 가지는 데이터)를 다루기에 최적화된 인공 신경망.
#   자연어 처리(NLP: Natural Language Processing) 문제에 주로 사용되는 인공신경망 구조.
#   그 외에도 주가, 파형으로 표현되는 음성 데이터 등이 있음.
#   ANN에서 은닉층에서 자기 자신으로 돌아가는 구간이 추가된 구조.
#   현재(t)의 결과가 다음(t+1)에 영향을 미치고, t+1의 결과가 그 다음(t+2)에 영향을 미치는 과정이 끊임없이 반복되는 구조.
#   이렇게 되면 이전 상태에 대한 정보를 일종의 메모리 형태로 저장할 수 있음.
# 경사도 사라짐 문제
#   오류 역전파 문제는 활성화 함수 중 relu를 사용함으로써 어느정도 해결이 가능했지만,
#   RNN의 경우는 조금 다른 측면에서 경사도 사라짐 문제가 있음.
#   t에서의 학습결과가 t+n번째의 학습들에 의해 점점 희석되는 것.
#   즉 장기 기억력을 가지지 못하다는 것.
#   이를 해결하기 위해 장/단기 기억 네트워크(LSTM: Long-Short Term Memory Network)가 제안됨.
# LSTM
#   은닉층의 각각의 노드를 인풋게이트, 포겟게이트, 아웃풋게이트로 구성된 메모리 블럭이라는 구조로 대체함.
#   포겟게이트를 통해 t-1의 학습결과를 받고, 인풋게이트를 통해 t의 학습결과를 받을 수 있을 때,
#   포겟게이트는 열고 인풋게이트는 닫을 경우, 과거의 데이터로 인한 학습결과가 현재의 데이터에 의해 희석되는 현상을 완화할 수 있음.
#   게이트는 완전이 닫힐 수도(0), 완전히 열릴 수도(1), 적절하게 열릴 수도(0~1) 있음. 최적화 대상임.
#   단, 아웃풋게이트까지 고려하면 연산량이 많아 무겁다는 단점이 존재함. 이를 해결하기 위해 GRU(Gate Recurrent Unit)이 제안 됨.
# GRU
#   아웃풋게이트를 제거한 경량화 버전의 LSTM.